<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://rl-max.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rl-max.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-26T05:11:26+00:00</updated><id>https://rl-max.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Summary of Transformer-based Architectures in Computer Vision</title><link href="https://rl-max.github.io/blog/2024/transformer-cv/" rel="alternate" type="text/html" title="A Summary of Transformer-based Architectures in Computer Vision"/><published>2024-02-26T00:00:00+00:00</published><updated>2024-02-26T00:00:00+00:00</updated><id>https://rl-max.github.io/blog/2024/transformer-cv</id><content type="html" xml:base="https://rl-max.github.io/blog/2024/transformer-cv/"><![CDATA[<p>In this article, we review modern vision models that are related to Transformer architecture. Spanning from vanilla ViT toward MetaFormer, we describe each architecture in detail with visual schematics and formulas.</p> <h2 id="vit">ViT</h2> <p><a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></p> <p>ViT successfully adapts transformer architecture in computer vision for the first time, outperforming other CNN-based architectures with fewer computational resources, while it is more specialized to classification tasks. The authors claim that to make such an architecture work, a pretraining on large amounts of data was necessary since transformer provides no inductive bias such as translation invariance in CNN, which might have eased the training on small datasets.</p> <p>ViT simply divides an image into multiple fixed-size patches (i.e. 16x16 in this paper) and treat those as same as word embeddings in NLP. Concretely, each patch is transformed into a vector by a linear layer and is processed by the Transformer encoder as follows.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/vit-480.webp 480w, /assets/img/vit-800.webp 800w, /assets/img/vit-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/vit.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="computer-visionn"/><category term="ViT"/><category term="CV"/><category term="ImageNet"/><summary type="html"><![CDATA[we review modern vision architectures related to transformer.]]></summary></entry></feed>