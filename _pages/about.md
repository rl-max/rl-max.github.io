---
layout: about
title: about
permalink: /
subtitle: haeone.lee@kaist.ac.kr

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p> üìç Seoul, Korea </p>
    <p> </p>
    <p> </p>

news: false  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---
<!-- Hi there, my name is Haeone Lee. My goal is to develop intelligence that is helpful to humans, consisting of any form e.g., physical embodiment(robots), or software(android agent). I believe in the power of **Reinforcement Learning**, in that sense (1) it can reach the optimal performance (2) it interacts with and adapts to the changing world (3) it is the closest to how animals ‚Äòemerge‚Äô the intelligence as part of goal pursuit. To make RL successful, I deem there are plenty of challenges to solve such as enabling efficient exploration, long-horizon control, and safe and autonomous learning. To this end, I am interested in utilizing prior knowledge(e.g., common sense, offline data), and equipping the algorithms with long-term memorizing, hierarchical decision-making, and good abstraction capabilities. For details, [**this**](https://rl-max.github.io/assets/pdf/Creating_Artificial_Intelligence_from_the_World.pdf) briefly surveys my thoughts. 

Hi there, my name is Haeone Lee. My goal is to develop intelligent agent that can outperform human, while also being helpful. I believe in the power of **Reinforcement Learning**, in that sense (1) it can autonomously come up with the solution given only the goal (2) it interacts with and adapts to the changing world (3) it is the closest to how animals ‚Äòemerge‚Äô the intelligence as part of goal pursuit. To make RL successful, I deem there are plenty of challenges to solve such as enabling efficient exploration, long-horizon control, and safe and autonomous learning. To this end, I am interested in utilizing prior knowledge(e.g., common sense, offline data), and equipping the algorithms with long-term memorizing, hierarchical decision-making, and good abstraction capabilities. For details, [**this**](https://rl-max.github.io/assets/pdf/Creating_Artificial_Intelligence_from_the_World.pdf) briefly surveys my thoughts. 

I am interested in building intelligent agents that can self-improve to be useful for humans. Specifically, it should generate useful problems and solve them by leveraging prior knowledge with critics to validate the success. I believe in the power of Reinforcement Learning, in that sense (1) it can autonomously come up with the solution given the goal (2) it interacts with and adapts to the changing world (3) it is the closest to how animals ‚Äòemerge‚Äô the intelligence as part of goal pursuit. To make RL successful, I deem there are plenty of challenges to solve such as sample efficiency, long-horizon control, and safe and autonomous learning. I believe that utilizing prior knowledge(e.g., common sense, offline data), and equipping the algorithms with long-term memorizing, hierarchical decision-making, and good abstraction can help to achieve my goal. For details, [**this**](https://rl-max.github.io/assets/pdf/Creating_Artificial_Intelligence_from_the_World.pdf) briefly surveys my thoughts. -->

I am a master student in KAIST Graduate school of AI, advised by Prof. [Kimin Lee](https://sites.google.com/view/kiminlee/home).

I am interested in developing a safe and proficient decision-making agent in real-world(e.g., robots). To this end, I aim to develop a method that can extract behavioral rules from existing data efficiently and help the agent continuously self-improve. Relevant topics include imitation learning on human data and developing scalable reinforcement learning (RL) algorithms that can work both on/offline. 

Relevant keywords include imitation learning, hierarchical RL, explorations in RL, and robot learning.

<!-- _pages/publications.md -->
<div class="publications">

{% bibliography -f {{ site.scholar.bibliography }} %}

</div>